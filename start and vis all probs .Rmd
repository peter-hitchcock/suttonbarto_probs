---
title: "Notebook to start and visualize all problems after Chapter 4"
output:
  html_document:
    df_print: paged
---


```{r setup}
lapply(c('tidyverse', 'dplyr', 'profvis'), require, character=TRUE)
sf <- function() sapply(paste0('./Functions/', list.files('./Functions/')), source)
sf()
DefPlotPars()
```

## Chapter 4

#### Solving Jack's car rental problem via policy iteration

```{r}
#### INITIALIZATIONS ####
locations <- c(1, 2) # rental car locations
n_customers <- NA # customers arriving each day
max_cars <- 20 # cars at a location can't exceed this amt
max_car_move <- 5
gamma <- .9 # discount
rent_lag <- 1 # days (states) need to wait before can return cars
policy <- rep(0, locations) # initialize at move no cars
state_values <- rep(0, locations) # initialize at no state values
theta <- 1e-3
############################################
ReturnLoc1 <- function(n) rpois(n, 3)
ReturnLoc2 <- function(n) rpois(n, 2)
RentLoc1 <- function(n) rpois(n, 3)
RentLoc2 <- function(n) rpois(n, 4)

TryToRent <- function(n_cars) money <- ifelse(n_cars > 0, 10, 0)
MoveCar <- function(current_loc, money) {
  #### Dock $2 and return new location for moving a car ####
  
  # new location is the other location
  new_loc <- ifelse(current_loc==1, 2, 1)
  money <- money-2

list(new_loc, money)     
}

# time steps=days
# states=# of cars at each location after day
# action=sum # of cars moved between locations

CalcSPrimeRewAndItsProb <- function(location, 
                                    cars_this_loc, 
                                    cars_other_loc, 
                                    money,
                                    max_cars,
                                    max_car_move) {
  
  ### Find the value of all policies from a given location ie state #
  
  # Jack can move 0:min(max_car_move...
  min(, max_car_move-5) cars
  
  
  
}

delta_vec <- 1e5

while (delta_vec > theta) {
  
}

for (state in seq_along(locations)) {
  old_value <- state_values[state]
  
}

```


## Chapter 5  

#### Translating pseudocode for first-visit MC p. 92

```{r}
policy <- c(.5, .5) # equiprobably visit every state
states <- matrix(1:10, 2, 5) # create 10 total arbitrarily indexed states states
# randomly assign rewards to the states
state_values <- matrix(runif(states, -10, 10), 2, 5) 
returns <- rep(NA, length(states)) # returns to fill with experience
gamma <- .8 # discount

#loop forever
while (1==1) {
  # generate an episode by following policy pi
  # ** to do:
  ## start at state 1
  # generate action|pi to get to next state
  # if (state %% 2) then with pi probabities +1/+2; if ! with pi probs +2/+3
  # collect reward from transition
  # .. keep moving
  
  time_steps <- 5 # for 5 time steps
  # can just dynamically create G in R
  
  # iterate thru time steps
  for (step in seq_along(time_steps)) {
    # .. discounting time step as you go 
    time_steps <- time_steps-1
    G <- G * gamma + reward[time_steps+1] # summed returns
    if (!states[time_step] %in% previous_states) { # ** where did we get previous states?
      returns[[this_state]] <- G # * we don't have this index yet
      state_values[this_state] <- mean(returns[[this_state]]) 
    }
  }

}

```

