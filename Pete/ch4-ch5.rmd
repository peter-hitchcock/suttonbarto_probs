---
title: "Chapters 4 and 5: DP and MC"
output:
  html_document:
    df_print: paged
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=14, fig.height=10) 


lapply(c('tidyverse', 'dplyr', 'profvis'), require, character=TRUE)
sf <- function() sapply(paste0('./Functions/', list.files('./Functions/')), source)
sf()
DefPlotPars()
```

## Chapter 4

#### Solving Jack's car rental problem via policy iteration

```{r}
#### INITIALIZATIONS ####
locations <- c(1, 2) # rental car locations
n_customers <- NA # customers arriving each day
max_cars <- 20 # cars at a location can't exceed this amt
max_car_move <- 5
gamma <- .9 # discount
rent_lag <- 1 # days (states) need to wait before can return cars
policy <- rep(0, locations) # initialize at move no cars
state_values <- rep(0, locations) # initialize at no state values
theta <- 1e-3
############################################
ReturnLoc1 <- function(n) rpois(n, 3)
ReturnLoc2 <- function(n) rpois(n, 2)
RentLoc1 <- function(n) rpois(n, 3)
RentLoc2 <- function(n) rpois(n, 4)

TryToRent <- function(n_cars) money <- ifelse(n_cars > 0, 10, 0)
MoveCar <- function(current_loc, money) {
  #### Dock $2 and return new location for moving a car ####
  
  # new location is the other location
  new_loc <- ifelse(current_loc==1, 2, 1)
  money <- money-2

list(new_loc, money)     
}

# time steps=days
# states=# of cars at each location after day
# action=sum # of cars moved between locations

CalcSPrimeRewAndItsProb <- function(other_state_value, 
                                    #cars_this_loc, 
                                    #cars_other_loc, 
                                    #money,
                                    max_cars,
                                    max_car_move) {
  
  ### Find the value of all policies from a given location ie state #
  
  # r is the amount from moving 
  # v(s) given by expected rentals during day
  
  # Jack can move 0:min(max_car_move...
  #min(, max_car_move-5) cars
  
  
  
}

delta_vec <- 1e5

while (delta_vec > theta) {
  
}

for (state in seq_along(locations)) {
  old_value <- state_values[state]
  
}

```

#### Gambler's problem

```{r}
lapply(c('tidyverse', 'dplyr'), require, character=TRUE)
sf <- function() sapply(paste0('./Functions/', list.files('./Functions/')), source)
sf()
DefPlotPars()
#### 4.9. Gambler's Problem ####
############################################
####  INITIALIZATIONS ####
theta <- 1e-5 # threshold param
gamma <- 1
states <- 1:101 # one state for 1:99 capital + 2 terminal states
p_win <- .15 # probability of winning 
# initalize state values randomly.. 
state_values <- runif(length(states), 0, 1)
#state_values <- rep(0, length(states))
# .. except for setting terminal states to 0 and 1
state_values[1] <- 0; state_values[101] <- 1
quiet <- 0 
keep_iter_ests <- 1
############################################
opt_policy_and_value_fx  <-  RunGamblersProblem(
                                            p_win, # probability of winning bet
                                            states, # = capital amounts
                                            state_values, # iteratively updating state values
                                            theta, # iteration stopping criterion
                                            gamma, # discount on V(s)'
                                            quiet, # print?
                                            keep_iter_ests # save estimates from all sweeps?
                                          ) 

# plots for final value and policy estimates
if (!keep_iter_ests) {
  ggplot(opt_policy_and_value_fx, aes(x=states, y=state_values)) + 
    geom_line(size=4, color='gray57') + 
    ga + ap +
    ylab('state values')
  
  ggplot(opt_policy_and_value_fx[[23]], aes(x=states, y=optimal_policy)) + 
    geom_point(size=4, alpha=.5) +
    ga + ap +
    ylab('optimal policy (amt. to stake)')
} 
if (keep_iter_ests) {
  
  ov_df <- opt_policy_and_value_fx %>% bind_rows()
  
  ggplot(ov_df, aes(x=states, y=state_values, color=as.factor(sweep))) + 
    geom_line(size=2, alpha=.9) + 
    ga + ap + 
    theme(legend.text = element_text(size = 8),
          legend.title = element_blank(),
          legend.key.size = unit(.8, 'lines')) +
    ylab('state values')
  
  ggplot(ov_df %>% filter(sweep %in% 94), aes(x=states, y=optimal_policy, color=as.factor(sweep))) + 
    geom_jitter(size=4, alpha=.6, width=.8, height=.8) +
    ga + ap + 
    theme(legend.text = element_text(size = 14),
          legend.title = element_blank(),
          legend.key.size = unit(2, 'lines')) +
    ylab('optimal policy (amt. to stake)') + ylim(0, 100)
}
```


## Chapter 5  

#### Race car problem 5.12

```{r}
MCSolveRaceTrack()
```





#### Translating pseudocode for first-visit MC p. 92

```{r}
policy <- c(.5, .5) # equiprobably visit every state
states <- matrix(1:10, 2, 5) # create 10 total arbitrarily indexed states states
# randomly assign rewards to the states
state_values <- matrix(runif(states, -10, 10), 2, 5) 
returns <- rep(NA, length(states)) # returns to fill with experience
gamma <- .8 # discount

#loop forever
while (1==1) {
  # generate an episode by following policy pi
  # ** to do:
  ## start at state 1
  # generate action|pi to get to next state
  # if (state %% 2) then with pi probabities +1/+2; if ! with pi probs +2/+3
  # collect reward from transition
  # .. keep moving
  
  time_steps <- 5 # for 5 time steps
  # can just dynamically create G in R
  
  # iterate thru time steps
  for (step in seq_along(time_steps)) {
    # .. discounting time step as you go 
    time_steps <- time_steps-1
    G <- G * gamma + reward[time_steps+1] # summed returns
    if (!states[time_step] %in% previous_states) { # ** where did we get previous states?
      returns[[this_state]] <- G # * we don't have this index yet
      state_values[this_state] <- mean(returns[[this_state]]) 
    }
  }

}

```















