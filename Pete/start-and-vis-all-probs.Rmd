---
title: "Notebook to start and visualize all problems after Chapter 4"
output:
  html_document:
    df_print: paged
---


```{r setup}
lapply(c('tidyverse', 'dplyr', 'profvis'), require, character=TRUE)
sf <- function() sapply(paste0('./Functions/', list.files('./Functions/')), source)
sf()
DefPlotPars()
```

## Chapter 4

#### Solving Jack's car rental problem via policy iteration

```{r}
#### INITIALIZATIONS ####
locations <- c(1, 2) # rental car locations
n_customers <- NA # customers arriving each day
max_cars <- 20 # cars at a location can't exceed this amt
max_car_move <- 5
gamma <- .9 # discount
rent_lag <- 1 # days (states) need to wait before can return cars
policy <- rep(0, locations) # initialize at move no cars
state_values <- rep(0, locations) # initialize at no state values
theta <- 1e-3
############################################
ReturnLoc1 <- function(n) rpois(n, 3)
ReturnLoc2 <- function(n) rpois(n, 2)
RentLoc1 <- function(n) rpois(n, 3)
RentLoc2 <- function(n) rpois(n, 4)

TryToRent <- function(n_cars) money <- ifelse(n_cars > 0, 10, 0)
MoveCar <- function(current_loc, money) {
  #### Dock $2 and return new location for moving a car ####
  
  # new location is the other location
  new_loc <- ifelse(current_loc==1, 2, 1)
  money <- money-2

list(new_loc, money)     
}

# time steps=days
# states=# of cars at each location after day
# action=sum # of cars moved between locations

CalcSPrimeRewAndItsProb <- function(other_state_value, 
                                    #cars_this_loc, 
                                    #cars_other_loc, 
                                    #money,
                                    max_cars,
                                    max_car_move) {
  
  ### Find the value of all policies from a given location ie state #
  
  # r is the amount from moving 
  # v(s) given by expected rentals during day
  
  # Jack can move 0:min(max_car_move...
  #min(, max_car_move-5) cars
  
  
  
}

delta_vec <- 1e5

while (delta_vec > theta) {
  
}

for (state in seq_along(locations)) {
  old_value <- state_values[state]
  
}

```


## Chapter 5  

#### Race car problem 5.12

```{r}
#### HELPER FUNCTIONS ####
## General and Initialization Fxs ## 
CMat2RC <- function(mat_ind, nrows=track_height) {
  ### Convert from matrix index to row, column indices ###
  
  rc <- data.frame("col"=NA, "row"=NA)
  if (mat_ind <= nrows) {
    # then we're in first column
    rc["col"] <- 1
    rc["row"] <- mat_ind
  } else {
    rc["col"] <- ceiling(mat_ind/nrows)
    rc["row"] <- ifelse(mat_ind %% 30==0, 30, mat_ind %% nrows)
  }
rc  
}
CRC2Mat <- function(rc, nrows=track_height) {
  ### Convert from row, column indices to matrix indices #
  mat_ind <- rc[2] * nrows + rc[1]
mat_ind  
}
InitTrack <- function(track_height) {
  ### Initialize track where 0s represent starting states, 2s finish states inclusive those 
  # past boundary, 1 intermediate states, and NAs off track and non-finish #
  
  track <- matrix(1, track_height, 19)
  track[track_height, ] <- 0
  track[, 15:19] <- 2
  track[1:10, 1:5] <- NA
  track[11:12, 1:3] <- NA
  track[13:14, 1:2] <- NA
  track[15, 1:2] <- NA
  track[15:track_height, 11:19] <- NA

track    
}
## Fxs for Generating an Episode ##
### Randomly pick a starting state index (used to start each ep) ###
GenEpStartState <- function(start_inds) start_inds[round(runif(1, 1, length(start_inds)))]
UpdateEpisode <- function(t,
                          velocity,
                          curr_state,
                          curr_hor_act,
                          curr_ver_act) {
  
     ### Update episode this time step ###
     
     episode$states[t] <- curr_state
     episode$hor_actions[t] <- curr_hor_act
     epsiode$ver_actions[t] <- curr_ver_act  
     episode$velocity[t] <- velocity

episode 
}
TransitionSAR <- function(track, # to find if actions-1|state-1 moved us is an intermed, off track or finish state
                          state_policy,
                          curr_state,
                          curr_velocity,
                          noise_velocity=0
                          ) {
     
     ###  Returns next SAR and velocity ###
     
     # Get row, column representation of state
     curr_state_rc <- curr_state_rc <- CMat2RC(curr_state) 
     
     # Generate action and current velocity
     # indicator to increment velocity 0 irrespective action cohosen
     incr_vel_0 <- ifelse(runif(1, 0, 1) < noise_velocity, 1, 0)
     if (t == 1) {
       hor_av <- SelectAction("hor", curr_velocity$hor, t, incr_vel_0)
       ver_av <- SelectAction("ver", curr_velocity$ver, t, incr_vel_0)
       curr_actions <- list("curr_hor_act"=hor_av$a, "curr_ver_act"=ver_av$a)
       curr_velocity <- list("curr_hor_vel"=hor_av$v, "curr_ver_vel"=ver_av$v)
       # velocity
     } else {
       # enforce constraint that at least 1 velocity is greater than 0
       while (all(curr_velocity)==0) {
         hor_av <- SelectAction("hor", curr_velocity$hor, t)
         ver_av <- SelectAction("ver", curr_velocity$ver, t)
         curr_actions <- list("curr_hor_act"=hor_av$a, "curr_ver_act"=ver_av$a)
         curr_velocity <- list("curr_hor_vel"=hor_av$v, "curr_ver_vel"=ver_av$v)
       }
     }
     
     # Use current velocity to find s'
     # Recalculate the matrix indices ..
     state_inds <- which(!is.na(track))
     # .. and row, column indices of all states
     state_rc_inds <- lapply(state_inds, CMat2RC) %>% bind_rows
     # indicator for if the row then col are on the track
     ot_row <- curr_state_rc$row + curr_velocity$curr_hor_vel %in% state_rc_inds$row
     ot_col <- curr_state_rc$col + curr_velocity$curr_ver_vel %in% state_rc_inds$col
      
     # ** don't think this is right because we want to include going off grid = -1 reward plus
     # go to starting state. for this should fxalize starting state
     # if the putative state transition is on the track find the locations of the next row and column
     if (ot_row & ot_col) {
       next_row <- curr_state_rc$row + curr_velocity$curr_hor_vel
       next_col <- curr_state_rc$col + curr_velocity$curr_ver_vel
     } else if (ot_row | ot_col) {
     # if only one is on track, find its row and column   
       if (!ot_row) next_row <- curr_state_rc$row + curr_velocity$curr_hor_vel
       if (!ot_col) next_col <- curr_state_rc$col + curr_velocity$curr_ver_vel
     }
     # for any not on track, set to max boundary
     if (!ot_col) next_row <- max(state_rc_inds$col)
     if (!ot_row) next_col <- max(state_rc_ind$row)  
       
     next_state <- list("r"=next_row, "c"=next_col)
     
     # Determine reward and if state transition is to a terminal state
     state_type <- track[next_row, next_col]
     
     # finish state
     if (state_type == 2) { reward <- 0; terminal <- 1 }
     # intermed state of start state
     if (state_type == 1 | state_type == 0) { reward <- -1; terminal <- 0 }
        
list("ns"=next_state, "nha"=next_hor_act, "nva"=next_ver_act, "rew"=reward, "vel"=velocity, "T"=terminal)   
}
SelectActionIncrVelocity <- function(dir_as_string, 
                                     velocity_this_dir,
                                     t,
                                     incr_vel_0=0) {
  
       ### Select horizontal / vertical action and increment velocity based on policy 
       # and s.t. constraints (see below) ###
       
       # velocity in any direction be nonnegative and can't exceed 5 and both velocities can't be 0  for
       # any state other than a starting state ###
       
       # Find cumulative probabilities of the 3 actions
       cum_probs <- cumsum(as.numeric(policy %>% 
                           filter(row==curr_state_rc$row & col==curr_state_rc$col) %>% 
                           select(contains(dir_as_string))))
       # Action selected is the first greater than a random number between 0 and 1
       action <- which(cum_probs > runif(1, 0, 1))[1]
       
       # Only change velocity if this indicator set to 0
       if (!incr_vel_0) {
          velocity_this_dir <- velocity_this_dir + action
         
         # Apply constraints:
         # must be nonnegative..
         if (velocity_this_dir < 0) { action <- 0; velocity_this_dir <- 0 } 
         # .. and less than 5
         if (velocity_this_dir > 5) { action <- 0; velocity_this_dir <- 5 } 
       } 
       
list("a"=action, "v"=velocity_this_dir)   
}
GenerateEpisode <- function(
                             # always initialize S_0 to starting states 
                             # bc those're what we want to find opt policies for
                             states_0, 
                             finish_inds,
                             actions_0, # 1 row data frame of horizontal, vertical actions
                             track, # track to find matrix indices of subsequent states 
                             policy, # policy to follow after initial actions
                             time_steps=15, # length of episode if boundary isn't reached first
                             noise_velocity=.1, # if nonzero, add this much noise to velocity
                             quiet=0
                             ) {
   
   ### Returns min(time step, bounary reached) S, A, R triples ###
   
   episode <- data.frame('states'=rep(NA, time_steps), 
                         'hor_actions'=rep(NA, time_steps), 
                         'ver_actions'=rep(NA, time_steps),
                         'rewards'=rep(NA, time_steps),
                         # track velocity for informational purposes
                         'velocity'=rep(NA, time_steps))
   
   
   # Set current state and actions to S_0 and A_0s
   t <- 1 # time step
   velocity <- 0
   # Pick current state randomly from set of starting states
   curr_state <- states_0[runif(1, 0, length(states_0))]
   curr_hor_act <- actions_0$hor
   curr_ver_act <- actions_0$ver
   curr_velocity <- list("hor"=0, "ver"=0) # initialize at 0 velocity
   
   terminal <- 0
   # Continue generating episode until a terminal state is reached or time steps run out
   while (terminal == 0 & t < time_steps) {
     
     # Get next state, actions, reward, velocities, and if this was a terminal state 
     SARVT <- TransitionSAR(track, 
                           actions,
                           curr_state,
                           curr_velocity,
                           noise_velocity
                           )
     episode <- UpdateEpisode(t, velocity, curr_state, curr_hor_act, curr_ver_act, episode)
   }

   
episode   
}
############################################

```


```{r}
MCSolveRaceTrack <- function(
                             gamma=.9,
                             MC_policy="Exploring Starts"="Exploring Starts",
                             quiet=0
                             ) {
  ### Solve the racetrack problem with Monte Carlo returning an optimal policy for 
  # each starting state ###
  
 #### INITIALIZATIONS ####
 # must be at least 16
 track_height <- 30
 track <- InitTrack(track_height)
 # vertical and horizontal current velocities (can never exceed 5 or both be 0 except at start)
 start_velocity <- data.frame('hor'=0, 'ver'=0)
 # matrix representation of action set (actions correspond to velocity increments)
 possible_actions <- expand.grid('hor'=c(-1, 0, 1), 'ver'=c(-1, 0, 1))
 n_states <- length(which(!is.na(track)))  
 curr_velocity <- start_velocity
 # must be at least 16
 track_height <- 30
 # matrix indices of all start points..
 start_inds <- which(track == 0)
 # .. row column indices of all start points ..
 start_rc_inds <- lapply(start_inds, CMat2RC) %>% bind_rows
 # .. finish line matrix inds and RC inds .. 
 finish_inds <- which(track == 2)
 finish_rc_inds <- lapply(finish_inds, CMat2RC) %>% bind_rows()
 # .. and valid state inds and RC inds 
 state_inds <- which(!is.na(track))
 state_rc_inds <- lapply(state_inds, CMat2RC) %>% bind_rows()
 ############################################  
 
 # Implement early starts policy (p.99). no others yet implemented
 if (MC_policy == "Exploring Starts") {
   
   # Initalize equal policy for each state where states ref'd by their row, col indices
   policy <- state_rc_inds 
   policy[, paste("hor", -1:1, sep="_")]  <- 1/3
   policy[, paste("ver", -1:1, sep="_")]  <- 1/3
   
   # Arbitrarily initialize Q vals with states again ref'd by row, col indices
   # Q_vals <- state_rc_inds
   # Q_vals[, paste("hor", -1:1, sep="_")] <- 1
   # Q_vals[, paste("ver", -1:1, sep="_")] <- 1
   
   # Preallocate dataframe with, for each state therein referenced by its matrix index representation 
   # (and all possible actions therefrom) q_vals, pi. 
   # Then episode by episode we'll add a column with the returns
   
   state_Q_pi_rets <- data.frame()
   for (s in seq_along(state_inds)) {
     this_row <- data.frame(
                            'state'=state_inds[s], 
                            poss_actions,
                            'Q_sa'=runif(nrow(possible_actions), -1, 1), # initialize arbitrary Q vals between 0 and 1 
                            'policy'=1/nrow(possible_actions) # initialize with equiprobable policy of acting in any state
                            ) 
     state_Q_pi_rets <- rbind(state_Q_pi_rets, this_row)
   }
   
   Q_vals <- data.frame()
   for (s in seq_along(state_inds)) {
     this_q_val <- data.frame('state'=state_inds[s], 
                              poss_actions, 
                              policy=1/nrow(possible_actions),
                              Q=runif(1, -1, 1)) 
     Q_vals <- rbind(Q_vals, this_q_val)
   }
   
   episode <- GenerateEpisode(states_0=start_inds, 
                              finish_inds, 
                              # randomly select first horiz and vertical actions from set of allowed actions
                              #actions_0=data.frame('hor'=a_mat$hor[round(runif(1, 1, 9))],
                              #                     'ver'=a_mat$ver[round(runif(1, 1, 9))]),
                              track,
                              policy)
   
   G <- 0 # returns
   
   
     for (t in 1:(episode$time_steps-1)) {
       G <- gamma * G + reward
       for (row in 1:nrow(episode)) {
        this_row <- episode[c("states", "hor_actions", "ver_actions")] 
       }
       
       
     
    }
 }
 
optimal_s0_policies
}
```











#### Translating pseudocode for first-visit MC p. 92

```{r}
policy <- c(.5, .5) # equiprobably visit every state
states <- matrix(1:10, 2, 5) # create 10 total arbitrarily indexed states states
# randomly assign rewards to the states
state_values <- matrix(runif(states, -10, 10), 2, 5) 
returns <- rep(NA, length(states)) # returns to fill with experience
gamma <- .8 # discount

#loop forever
while (1==1) {
  # generate an episode by following policy pi
  # ** to do:
  ## start at state 1
  # generate action|pi to get to next state
  # if (state %% 2) then with pi probabities +1/+2; if ! with pi probs +2/+3
  # collect reward from transition
  # .. keep moving
  
  time_steps <- 5 # for 5 time steps
  # can just dynamically create G in R
  
  # iterate thru time steps
  for (step in seq_along(time_steps)) {
    # .. discounting time step as you go 
    time_steps <- time_steps-1
    G <- G * gamma + reward[time_steps+1] # summed returns
    if (!states[time_step] %in% previous_states) { # ** where did we get previous states?
      returns[[this_state]] <- G # * we don't have this index yet
      state_values[this_state] <- mean(returns[[this_state]]) 
    }
  }

}

```















