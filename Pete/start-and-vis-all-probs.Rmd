---
title: "Notebook to start and visualize all problems after Chapter 4"
output:
  html_document:
    df_print: paged
---


```{r setup}
lapply(c('tidyverse', 'dplyr', 'profvis'), require, character=TRUE)
sf <- function() sapply(paste0('./Functions/', list.files('./Functions/')), source)
sf()
DefPlotPars()
```

## Chapter 4

#### Solving Jack's car rental problem via policy iteration

```{r}
#### INITIALIZATIONS ####
locations <- c(1, 2) # rental car locations
n_customers <- NA # customers arriving each day
max_cars <- 20 # cars at a location can't exceed this amt
max_car_move <- 5
gamma <- .9 # discount
rent_lag <- 1 # days (states) need to wait before can return cars
policy <- rep(0, locations) # initialize at move no cars
state_values <- rep(0, locations) # initialize at no state values
theta <- 1e-3
############################################
ReturnLoc1 <- function(n) rpois(n, 3)
ReturnLoc2 <- function(n) rpois(n, 2)
RentLoc1 <- function(n) rpois(n, 3)
RentLoc2 <- function(n) rpois(n, 4)

TryToRent <- function(n_cars) money <- ifelse(n_cars > 0, 10, 0)
MoveCar <- function(current_loc, money) {
  #### Dock $2 and return new location for moving a car ####
  
  # new location is the other location
  new_loc <- ifelse(current_loc==1, 2, 1)
  money <- money-2

list(new_loc, money)     
}

# time steps=days
# states=# of cars at each location after day
# action=sum # of cars moved between locations

CalcSPrimeRewAndItsProb <- function(other_state_value, 
                                    #cars_this_loc, 
                                    #cars_other_loc, 
                                    #money,
                                    max_cars,
                                    max_car_move) {
  
  ### Find the value of all policies from a given location ie state #
  
  # r is the amount from moving 
  # v(s) given by expected rentals during day
  
  # Jack can move 0:min(max_car_move...
  #min(, max_car_move-5) cars
  
  
  
}

delta_vec <- 1e5

while (delta_vec > theta) {
  
}

for (state in seq_along(locations)) {
  old_value <- state_values[state]
  
}

```


## Chapter 5  

#### Race car problem 5.12

```{r}
#### HELPER FUNCTIONS ####
## General and Initialization Fxs ## 
CMat2RC <- function(mat_ind, nrows=track_height) {
  ### Convert from matrix index to row, column indices ###
  
  rc <- data.frame("col"=NA, "row"=NA)
  if (mat_ind <= nrows) {
    # then we're in first column
    rc["col"] <- 1
    rc["row"] <- mat_ind
  } else {
    rc["col"] <- ceiling(mat_ind/nrows)
    rc["row"] <- ifelse(mat_ind %% 30==0, 30, mat_ind %% nrows)
  }
rc  
}
CRC2Mat <- function(rc, nrows=track_height) {
  ### Convert from row, column indices to matrix indices #
  mat_ind <- rc[2] * nrows + rc[1]
mat_ind  
}
InitTrack <- function(track_height) {
  ### Initialize track where 0s represent starting states, 2s finish
  # states, 1 intermediate states, and NAs off track and non-finish #
  
  track <- matrix(1, track_height, 15)
  track[track_height, ] <- 0
  track[, 15] <- 2
  track[1:10, 1:5] <- NA
  track[11:12, 1:3] <- NA
  track[13:14, 1:2] <- NA
  track[15, 1:2] <- NA
  track[15:track_height, 11:15] <- NA

track    
}
## Fxs for Generating an Episode ##
### Randomly pick a starting state index (used to start each ep) ###
GenEpStartState <- function(start_inds) start_inds[round(runif(1, 1, length(start_inds)))]
UpdateEpisode <- function(t,
                          velocity,
                          curr_state,
                          curr_hor_act,
                          curr_ver_act) {
  
     ### Update episode this time step ###
     
     episode$states[t] <- curr_state
     episode$hor_actions[t] <- curr_hor_act
     epsiode$ver_actions[t] <- curr_ver_act  
     episode$velocity[t] <- velocity

episode 
}
TransitionSAR <- function(track, # to find if actions-1|state-1 moved us is an intermed, off track or finish state
                          state_policy,
                          curr_state,
                          curr_velocity
                          ) {
     
     ###  Returns next SAR and velocity ###
     
     next_state <- a
       
     
list("ns"=next_state, "nha"=next_hor_act, "nva"=next_ver_act, "rew"=reward, "vel"=velocity)   
}
SelectActionIncrVelocity <- function(dir_as_string, 
                                          velocity_this_dir,
                                          t,
                                          incr_vel_0=0) {
       ### Select horizontal / vertical action and increment velocity based on policy 
       # and s.t. constraints (see below) ###
       
       # velocity in any direction be nonnegative and can't exceed 5 and both velocities can't be 0  for
       # any state other than a starting state ###
       
       # find cumulative probabilities of the 3 actions
       cum_probs <- cumsum(as.numeric(policy %>% 
                           filter(row==curr_state_rc$row & col==curr_state_rc$col) %>% 
                           select(contains(dir_as_string))))
       # action selected is the first greater than a random number between 0 and 1
       action <- which(cum_probs > runif(1, 0, 1))[1]
       
       # only change velocity if  this indicator set to 0
       if (!incr_vel_0) {
          velocity_this_dir <- velocity_this_dir + action
         
         # Apply constraints:
         # Must be nonnegative..
         if (velocity_this_dir < 0) { action <- 0; velocity_this_dir <- 0 } 
         # .. and less than 5
         if (velocity_this_dir > 5) { action <- 0; velocity_this_dir <- 5 } 
       } 
       
     list("a"=action, "v"=velocity_this_dir)   
}
GenerateEpisode <- function(
                             # always initialize S_0 to starting states bc they're what we want 
                             # to derive policy for
                             states_0, 
                             finish_inds,
                             actions_0, # 1 row data frame of horizontal, vertical actions
                             # track to find matrix indices of subsequent states 
                             track, 
                             # policy to follow after initial actions
                             policy, 
                             # length of episode if boundary isn't reached first
                             time_steps=15,
                             noise_velocity=.1, # if nonzero, add this much noise to velocity
                             quiet=0
                             ) {
   
   ### Returns min(time step, bounary reached) S, A, R triples ###
   
   episode <- data.frame('states'=rep(NA, time_steps), 
                         'hor_actions'=rep(NA, time_steps), 
                         'ver_actions'=rep(NA, time_steps),
                         'rewards'=rep(NA, time_steps),
                         # track velocity for informational purposes
                         'velocity'=rep(NA, time_steps))
   
   
   # set current state and actions to S_0 and A_0s
   t <- 1 # time step
   velocity <- 0
   # pick current state randomly from set of starting states
   curr_state <- states_0[runif(1, 0, length(states_0))]
   curr_hor_act <- actions_0$hor
   curr_ver_act <- actions_0$ver
   curr_velocity <- list("hor"=0, "ver"=0) # initialize at 0 velocity
   
   terminal <- 0
   # continue generating episode until a terminal state is reached or time steps run out
   while (terminal == 0 & t < time_steps) {
     # get row, column representation of state
     curr_state_rc <- curr_state_rc <- CMat2RC(curr_state) 
     
     # indicator to increment velocity 0 irrespective action cohosen
     incr_vel_0 <- ifelse(runif(1, 0, 1) < noise_velocity, 1, 0)
     if (t == 1) {
       hor_av <- SelectAction("hor", curr_velocity$hor, t, incr_vel_0)
       ver_av <- SelectAction("ver", curr_velocity$ver, t, incr_vel_0)
       curr_actions <- list("curr_hor_act"=hor_av$a, "curr_ver_act"=ver_av$a)
       curr_velocity <- list("curr_hor_vel"=hor_av$v, "curr_ver_vel"=ver_av$v)
       # velocity
     } else {
       # enforce constraint of at least 1 velocity greater than 0
       while (all(curr_velocity)==0) {
         hor_av <- SelectAction("hor", curr_velocity$hor, t)
         ver_av <- SelectAction("ver", curr_velocity$ver, t)
         curr_actions <- list("curr_hor_act"=hor_av$a, "curr_ver_act"=ver_av$a)
         curr_velocity <- list("curr_hor_vel"=hor_av$v, "curr_ver_vel"=ver_av$v)
       }
     }
     
     SARV <- TransitionSAR(track, 
                           actions,
                           curr_state,
                           curr_velocity
                           )
     episode <- UpdateEpisode(t, velocity, curr_state, curr_hor_act, curr_ver_act, episode)
   }

   
episode   
}
############################################

```


```{r}
MCSolveRaceTrack <- function(
                             MC_policy="Exploring Starts"="Exploring Starts",
                             quiet=0
                             ) {
  ### Solve the racetrack problem with Monte Carlo returning an optimal policy for 
  # each starting state ###
  
 #### INITIALIZATIONS ####
 # must be at least 16
 track_height <- 30
 track <- InitTrack(track_height)
 # vertical and horizontal current velocities (can never exceed 5 or both be 0 except at start)
 start_velocity <- data.frame('hor'=0, 'ver'=0)
 # matrix representation of action set where actions corresopnd to changes in 
 a_mat <- expand.grid('hor'=c(-1, 0, 1), 'ver'=c(-1, 0, 1))
 n_states <- length(which(!is.na(track)))  
 curr_velocity <- start_velocity
 # must be at least 16
 track_height <- 30
 # matrix indices of all start points..
 start_inds <- which(track == 0)
 # .. row column indices of all start points ..
 start_rc_inds <- lapply(start_inds, CMat2RC) %>% bind_rows
 # .. finish line matrix inds and RC inds .. 
 finish_inds <- which(track == 2)
 finish_rc_inds <- lapply(finish_inds, CMat2RC) %>% bind_rows()
 # .. and valid state inds and RC inds 
 state_inds <- which(!is.na(track))
 state_rc_inds <- lapply(state_inds, CMat2RC) %>% bind_rows()
 ############################################  
 
 # Implement early starts policy (p.99). no others yet implemented
 if (MC_policy == "Exploring Starts") {
   
   # Initalize equal policy for each state where states ref'd by their row, col indices
   policy <- state_rc_inds 
   policy[, paste("hor", -1:1, sep="_")]  <- 1/3
   policy[, paste("ver", -1:1, sep="_")]  <- 1/3
   
   # Arbitrarily initialize Q vals with states again ref'd by row, col indices
   Q_vals <- state_rc_inds
   Q_vals[, paste("hor", -1:1, sep="_")] <- 1
   Q_vals[, paste("ver", -1:1, sep="_")] <- 1
   
   returns <- list()
   
   episode <- GenerateEpisode(states_0=start_inds, 
                              finish_inds, 
                              # randomly select first horiz and vertical actions from set of allowed actions
                              #actions_0=data.frame('hor'=a_mat$hor[round(runif(1, 1, 9))],
                              #                     'ver'=a_mat$ver[round(runif(1, 1, 9))]),
                              track,
                              policy)
   
   
 }
 
 
optimal_policy_each_state   
}
```











#### Translating pseudocode for first-visit MC p. 92

```{r}
policy <- c(.5, .5) # equiprobably visit every state
states <- matrix(1:10, 2, 5) # create 10 total arbitrarily indexed states states
# randomly assign rewards to the states
state_values <- matrix(runif(states, -10, 10), 2, 5) 
returns <- rep(NA, length(states)) # returns to fill with experience
gamma <- .8 # discount

#loop forever
while (1==1) {
  # generate an episode by following policy pi
  # ** to do:
  ## start at state 1
  # generate action|pi to get to next state
  # if (state %% 2) then with pi probabities +1/+2; if ! with pi probs +2/+3
  # collect reward from transition
  # .. keep moving
  
  time_steps <- 5 # for 5 time steps
  # can just dynamically create G in R
  
  # iterate thru time steps
  for (step in seq_along(time_steps)) {
    # .. discounting time step as you go 
    time_steps <- time_steps-1
    G <- G * gamma + reward[time_steps+1] # summed returns
    if (!states[time_step] %in% previous_states) { # ** where did we get previous states?
      returns[[this_state]] <- G # * we don't have this index yet
      state_values[this_state] <- mean(returns[[this_state]]) 
    }
  }

}

```















