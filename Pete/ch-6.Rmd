---
title: "Chapter 6"
output: html_notebook
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=14, fig.height=10) 


sapply(c('tidyverse', 'dplyr', 'profvis'), require, character=TRUE)
sf <- function() sapply(paste0('./Functions/', list.files('./Functions/')), source)
sf()
DefPlotPars()
```

# Book notes 

TD learning combines elements of MC and DP in that can learn directly from experience like the former wo a model but bootstrapping like the former. Various formulations can bridge fully between these methods such as n-step methods and $TD(\lambda)$.  

We've already covered MC. In the past chapter, one update for it was policy eval where:
$Q(s_t, a_t) = average(Returns(s_t, a_t))$ and returns were a running list of G, which were in turn updated as
$\gamma G + R_t$ for eg every visit in the episode.  

A good MC value fx estimate for nonstationary environments is  

$$V(S_t) = V(S_t) + \alpha[G_t - V(S_t)]$$
 
 me: or for action values I think would write  
 
 $$Q(S_t, A_t) = Q(S_t, A_t) + \alpha[G_t - Q(S_t, A_t)$$
 
The key difference with the earlier MC update rules presented for stationary environments is that instead of taking an unweighted average is uses a learning rate update and so is more myopic with higher $\alpha$ which presumably makes it better for nonstationary environments. However, it's still MC because it waits until the end of the episode to do any udpating to the value estimate.  

## TD(0)

**The key distinction of TD is that it updates immediately instead of waiting to reach the end of the episode.**  

Instead of taking the difference between the full return $G_t$ which is the that-episode estimate of $V(S_t)$ and the info learned before the episode, it uses the immediate reward + the value of the next state. That this can replace using counting up and averaging all the subsequent returns (as in MC) comes from the recursive unrolling of the Bellman eqn from which we learned that the curren state estimate is nothing more than the immediate reward and next state value. Because the next state value is often only an estimate, this is an update based on a guess. The advantage of TD methods come from that guess still being better than having to wait until all the data are in to start guessing in the right direction.  

The simplest TD method $TD(0)$ uses the update

$$V(S_t) = V(S_t) + \alpha[r_{t+1} + \gamma V(s_{t+1}) - V(s_t)] $$



this also gives rise to our first PE: 

$$ \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t) $$
A Markov Reward Process is an MDP without actions (me: ie. Pavlovian?). They're often useful for prediction problems.  

## SARSA  

Moving to focus on using TD estimates for the control problem. This follows the same Generalized Policy Iteration framework as before just using TD for eval/prediction. Now instead of estimating state values we want to estimates action values. For evaluating a policy via an on-policy method we try to estimate $q_{\pi}(s,a)$. We've already talked about how to estimate state values and this turns out to be basically the same; rewards always interleave transitions so it's just S-R-S' vs SA-R-S'A' that we use for updates. SARSA is so-called because it uses the whole latter quintuple for updates  

$$ Q(S_t, A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t, A_t)] $$

## Q-learning

**The key difference from SARSA is that Q-learning uses the max action estimate from $Q(S_{t+1})$ for the prediction error, enabling a direct estimate of $q*$ the optimal value function.** 

$$ Q(S_t, A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma \ max_a \ Q(S_{t+1},a) - Q(S_t, A_t)] $$

Critically, this decouples the value estimation from the policy being followed.  

Besides taking the max, the key differences algorithmically are that you can take the first action in the loop through time steps because you're updating based on the results of action $A$ (+ reward + max value) rather than the update depending on having taken $A'$. For the same reason, you don't update $A \gets A'$ at the end, as instead of coupling your action steps to your changing value estimate, you are selecting actions anew based on the value estimate.  

*6.11: Why is Q-learning off-policy?*  
*A:* Except in the special case of a deterministic greedy policy, the actions actually taken online are not the same as those used to update the policy offline. Although the specific algorithms are different, this makes it analogous to the off-policy cases we discussed last chapter where we followed some behavioral policy but then updated our value function in a different way (in that case via some importance sampling scheme determining how relevant the behavior taken was to the policy we were estimating, in this case via the estimate of the max of $Q(S_{t+1} A_{t+1})$). 

As a consequence of decoupling its actions from what it estimates to be best, Q-learning can even asymptotically behave badly. In the cliff-walking example, Q-learning learns $q*$ offline but online it acts $\epsilon$-greedily. So it learns if it will only follow $q*$ but in practice falls short of this, leading to dire consequences. In contrast, online SARSA's update is based directly on A', so it will learn the less optimal policy of staying away from the cliff edge, which works out better for it when in practice it doesn't follow a deterministic action policy. 

*6.12* As $0 \gets \epsilon$, these are the same algorithm because $A \gets A'$ in SARSA is based on the max, leading the same number to be used in the updating.  











